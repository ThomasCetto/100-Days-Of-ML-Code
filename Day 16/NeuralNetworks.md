# Neural Networks 

A neural network (NN) is a set of layers made of small nodes (or neurons) that perform mathematical operations to detect patterns in data. NN algorithms try to mimic how human neurons work. 

### Key terms
1. Neuron or node, perceptron, unit: basic building block of a NN. It takes weighted values, performs mathematical calculation and produce output
2. Deep Neural Network (DNN): a NN with many hidden layers 
3. Weights: a value that explains the importance of the connection between any two neurons
4. Bias: a costant value added to the sum of the product between input values and respective weights.
5. Activation function: a function that is applied to the output of a neuron, allows the network to laern more complex patterns. It takes the weighted sum of the inputs to the neuron and applies a non-linear transformation to the result. The output of the function is then passed as input to the next layer in the NN.

### Neuron or Node
It takes input values with weights assidgned to them. Then the weighted inputs are summed up and an activation function is applied to get the results. The output of the node is passed to the other layers.


### Neural Network Design
A NN is made of several neurons stacked into layers. For a n-dimensional input, the first layer will have n nodes, while the other layers could have a different number of nodes.
All intermediate layers are called hidden layers, and the number of them determines the depth of the model.


## ____ Types of Neural Networks ____

### Feed-Forward Neural Network (FF-NN)
It's a type on NN where the flow of information moves in only one direction, from the input layer to the output layer.

There are no loops or cycles, so the output of one layer is not fed back into the same layer or any previous layer. It is the simpler type of NN.

### Convolutional Neural Networks (CNN)
It processes images by applying a series of convolutional filters to it. Those filters are small matrices that slide over the image and perform a computation at each position. The computation involves multiplying the values in the filter with the values in the corresponding part of the image and summing the results to produce a single output value. The output values are then passed through an activation function.

The filters are designed to detect specific features in the image, such as edges corners and textures. The filters are learned during training by adjusting  the weights and biases of the connections between the neurons, to minimize the error.

CNNs also use a technique called pooling, which reduces the size of the feature maps produced by the filters. This helps to reduce the number of parameters in the NN  and prevent overfitting. 

The output of the last layer is a set of probabilities for each possible class. The class with the highest probability is then selected as the predicted output.

It's designed to work with images and other multidimensional data. Commonly used in computer vision application such as object detection and image segmentation.

### Recurrent Neural Networks (RNN)
It's designed to precess sequential data, where the order of the data points is important, such as with time-series data.

RNNs have a recurrent connection that allows information to be passed from one time step to the next, allowing the network to mantain a "memory" of previous inputs. This is achieved by feeding the output of the network from the previous time step back as input to the netwenrk at the current time step.

During each time step, the network processes the current input and the previouus hidden state, and produces an output and a new hidden state. The output at each time step can be used as input for the next time step. 

RNNs are used for language modeling, speech recognition and sentiment analysis. They are particoularly well-suited to tasks that involve sequential or time-series data, where the context of previous inputs is important for making accurate predictions.


### Generative Adversarial Networks (GAN)
They consist of two networks, a generator and a discriminator, that are trained in an adversarial setting ("impostazione contraddittoria").
The generator network takes a random noise vector as input and generates a new data sample that is intended to be similar to the training data. The discriminator network takes a data sample as input and outputs a probability that the sample is real (the one from the training data) or fake (that is generated by the generator).

During training, the generator and discriminator netwerks are trained in an adversarial game, where the generator tries to produce samples that fool the discriminator into thinking they are real, while the discriminator tries to distinguish between the real and fake samples. The generator is updated to produce samples that are more difficult for the didscriminator to distinguish, while the discriminator is updated to better distinguish between real and fake samples.

The goal of training a GAN is to learn a generator network that can produce new samples that are indistinguishable from the the training data. Once the generator has been trained, it can be used to generate new data samples that are similar to the training data, but not identical.

They are used for image generation, style transfer and text generation. Trainng GANs can be challenging, and requires careful tuning of the hyperparameters and careful management of the training dynamics.



### Long short-term memory networks (LSTM)
A type of RNN that are designed to overcome the vanishing gradient problem, which can occur in traditional RNNs (while training, the gradients used to update the weights become very small as they propagate backwards through the network, and it becomes hard to update the weights in a meaningful way).

They use a series of memory cells that can store informration over long periods of time, along with three gates that control the flow of information into and out of the memory cells (input gate, forget gare and output gate).
The allow the netweok to selectively update and forget information.

During each time step, the input is combined with the previous hidden state, and passed through the input gate to update the memory cell. The forget gate determines which information to keep and which to discard from the memory cell, and the output gate determines which information to output from the memory cell to the next hidden state and the ouput.

 Commonly used in applications such as speech recognition, natural language processing and handwriting recognition.

### Autoencoders
Used to reconstruct the input data from a compressed representation of it. They are used for dimensionality reduction, data compression and anomaly detection.